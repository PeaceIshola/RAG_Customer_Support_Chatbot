{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568f5f07",
   "metadata": {},
   "source": [
    "# Customer‚ÄëSupport Chatbot for an E-Commerce Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46b9cc",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "We will build a RAG-based chatbot in **six** steps:\n",
    "\n",
    "1. **Environment setup**\n",
    "2. **Data preparation**  \n",
    "   a. Load source documents  \n",
    "   b. Chunk the text  \n",
    "3. **Build a retriever**  \n",
    "   a. Generate embeddings  \n",
    "   b. Build a FAISS vector index  \n",
    "4. **Build a generation engine**. Load the *Gemma3-1B* model through Ollama and run a sanity check.  \n",
    "5. **Build a RAG**. Connect the system prompt, retriever, and LLM together. \n",
    "6. **Streamlit UI**. Wrap everything in a simple web app so users can chat with the bot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01819c",
   "metadata": {},
   "source": [
    "## 1‚ÄØ-‚ÄØEnvironment setup\n",
    "\n",
    "We use conda to manage our project dependencies and ensure everyone has a consistent setup. Conda is an open-source package and environment manager that makes it easy to install libraries and switch between isolated environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a29e43",
   "metadata": {},
   "source": [
    "Let's import required libraries and print a message if we're not **missing packages**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b49fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries for file handling and text processing\n",
    "import os, pathlib, textwrap, glob\n",
    "\n",
    "import os, pathlib, textwrap, glob\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader, PyPDFLoader\n",
    "\n",
    "# Load documents from various sources (URLs, text files, PDFs)\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader, PyPDFLoader\n",
    "\n",
    "# Split long texts into smaller, manageable chunks for embedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector store to store and retrieve embeddings efficiently using FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Generate text embeddings using OpenAI or Hugging Face models\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "# Use local LLMs (e.g., via Ollama) for response generation\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Build a retrieval chain that combines a retriever, a prompt, and an LLM\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create prompts for the RAG system\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"‚úÖ Libraries imported! You're good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaacfdc",
   "metadata": {},
   "source": [
    "## 2‚ÄØ-‚ÄØData preparation\n",
    "The goal of this step is to turn all reference documents into small chunks of text that a retriever can index and search. These documents typically come from:\n",
    "* PDF files: local documents such as policies, user manuals, or guides.\n",
    "* Web pages (HTML): online documentation, blog posts, or help articles.\n",
    "\n",
    "In this step, we perform two actions:\n",
    "* **Ingesting**: load every PDF and collect the raw text in a list named `raw_docs`.\n",
    "* **Chunking**: split each document into small, overlapping chunks so later steps can match a user query to the most relevant passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = glob.glob(\"data/Everstorm_*.pdf\")\n",
    "raw_docs = []\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    raw_docs.extend(PyPDFLoader(pdf_path).load())\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} PDF pages from {len(pdf_paths)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f135e",
   "metadata": {},
   "source": [
    "### 2.1 - Load web pages\n",
    "You can also pull content straight from the web. Various libraries support reading and parsing web pages directly into text, which is useful for building custom knowledge bases. One example is **UnstructuredURLLoader** from LangChain, which can extract readable content from raw HTML pages and return them in a structured format. To learn more, see: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html\n",
    "\n",
    "To practice, load each HTML page below and store the results in a list called `raw_docs`. We‚Äôve included a few sample URLs, but you can replace them with any links you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = [\n",
    "    # --- BigCommerce ‚Äì shipping & refunds ---\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/shipping\",\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/orders/refunds\",\n",
    "    # --- Stripe ‚Äì disputes & chargebacks ---\n",
    "    # \"https://docs.stripe.com/disputes\",  \n",
    "    # --- WooCommerce ‚Äì REST API reference ---\n",
    "    # \"https://woocommerce.github.io/woocommerce-rest-api-docs/v3.html\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    loader = UnstructuredURLLoader(urls=URLS)\n",
    "    raw_docs.extend(loader.load())\n",
    "    print(f\"Fetched {len(raw_docs)} documents from the web.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è  Web fetch failed, using offline copies:\", e)\n",
    "    raw_docs = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        raw_docs.extend(PyPDFLoader(pdf_path).load())\n",
    "    print(f\"Loaded {len(raw_docs)} offline documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac4490",
   "metadata": {},
   "source": [
    "### 2.2‚ÄØ-‚ÄØChunk the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c242e44",
   "metadata": {},
   "source": [
    "Long documents won‚Äôt work well directly with most LLMs. They can easily exceed the model‚Äôs context window, making it impossible for the model to read or reason over the full text at once. Even if they fit, processing long inputs can be inefficient and lead to weaker retrieval results.\n",
    "\n",
    "To handle this, we split large documents into smaller, overlapping chunks. Several libraries can help with text splitting, each designed to preserve structure or balance chunk size. A popular choice is `RecursiveCharacterTextSplitter` from LangChain, which splits text intelligently while keeping paragraph or sentence boundaries intact. To familiarize youself with the library, visit: https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d48a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "print(f\"‚úÖ {len(chunks)} chunks ready for embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e6213",
   "metadata": {},
   "source": [
    "## 3¬†-Build a retriever\n",
    "\n",
    "A *retriever* lets the RAG pipeline efficiently look up small, relevant pieces of context at query‚Äëtime. This step has two parts:\n",
    "1. **Load a model to generate embeddings**: convert each text chunk from the reference documents into a fixed‚Äëlength vector that captures its semantic meaning.  \n",
    "2. **Build vector database**: store these embeddings in a vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90db2d2",
   "metadata": {},
   "source": [
    "### 3.1‚ÄØ- Load a model to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a222122",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = []\n",
    "\n",
    "# Embed the sentence \"Hello world! and store it in an embedding_vector.\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "embedding_vector = embeddings.embed_query(\"Hello world!\")\n",
    "print(len(embedding_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae3d",
   "metadata": {},
   "source": [
    "### 3.2‚ÄØ-‚ÄØBuild a vector database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected steps:\n",
    "    # 1. Build the FAISS index from the list of document chunks and their embeddings.\n",
    "    # 2. Create a retriever object with a suitable k value (e.g., 8).\n",
    "    # 3. Save the vector store locally (e.g., under \"faiss_index\").\n",
    "    # 4. Print a short confirmation showing how many embeddings were stored.\n",
    "\n",
    "vectordb = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "vectordb.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"‚úÖ Vector store with\", vectordb.index.ntotal, \"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456f820",
   "metadata": {},
   "source": [
    "## 4¬†-¬†Build the generation engine\n",
    "At the core of any RAG system lies an **LLM**. The retriever finds relevant information, and the LLM uses that information to generate coherent, context-aware responses.\n",
    "\n",
    "In this project, we‚Äôll use **Gemma 3* (1B), a small but capable open-weight model, and run it entirely on your local machine using Ollama. This means you won‚Äôt need API keys or internet access to generate responses once the model is downloaded.\n",
    "\n",
    "\n",
    "### 4.1 - Install `ollama` and serve `gemma3`\n",
    "\n",
    "Follow these steps to set up Ollama and start the model server:\n",
    "\n",
    "**1 - Install**\n",
    "```bash\n",
    "# macOS (Homebrew)\n",
    "brew install ollama\n",
    "# Linux\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "If you‚Äôre on Windows, install using the official installer from https://ollama.com/download.\n",
    "\n",
    "**2 - Start the Ollama server (keep this terminal open)**\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "This command launches a local server at http://localhost:11434, which will stay running in the background.\n",
    "\n",
    "\n",
    "**3 - Pull the Gemma mode (or the model of your choice) in a new terminal**\n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "```\n",
    "\n",
    "This downloads the 1B version of Gemma 3, a compact model suitable for running on most modern laptops. Once downloaded, Ollama will automatically handle model loading and caching.\n",
    "\n",
    "\n",
    "After this setup, your system is ready to generate responses locally using the Gemma model through the Ollama API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7e203",
   "metadata": {},
   "source": [
    "### 4.2 - Test an LLM with a random prompt (Sanity check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d34a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected steps:\n",
    "    # 1. Initialize the model (for example, gemma3:1b) with a low temperature such as 0.1 for more factual outputs.\n",
    "    # 2. Use llm.invoke() with a short test prompt and print the response to verify that the model runs successfully.\n",
    "\n",
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "print(llm.invoke(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556263d",
   "metadata": {},
   "source": [
    "## Build a RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfec15",
   "metadata": {},
   "source": [
    "### 5.1‚ÄØ-‚ÄØDefine a system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5c84b",
   "metadata": {},
   "source": [
    "At this stage, we need to tell the model how to behave when generating answers. The **system prompt** acts as the model‚Äôs rulebook. It should clearly instruct the model to answer only using the retrieved context and to admit when it doesn‚Äôt know the answer. This helps prevent hallucination and keeps the responses grounded in the provided documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a **Customer Support Chatbot**. Use only the information in CONTEXT to answer.\n",
    "If the answer is not in CONTEXT, respond with ‚ÄúI'm not sure from the docs.‚Äù\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY the provided <context> to answer.\n",
    "2) If the answer is not in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "3) Be concise and accurate. Prefer quoting key phrases from the context.\n",
    "4) When possible, cite sources as [source: source] using the metadata.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27cc9a",
   "metadata": {},
   "source": [
    "### 5.2 Create a RAG chain\n",
    "Now that we have a retriever, a prompt, and a language model, we can connect them into a single RAG pipeline. The retriever finds the most relevant chunks from our vector index, the prompt injects those chunks into the system message, and the LLM uses that context to produce the final answer. (retriever ‚Üí prompt ‚Üí model)\n",
    "\n",
    "This connection is handled through LangChain‚Äôs `ConversationalRetrievalChain`, which combines retrieval and generation. To familiarize yourself with the library, visit: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b138b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected steps:\n",
    "    # 1. Create a PromptTemplate that uses the SYSTEM_TEMPLATE you defined earlier, with input variables for \"context\" and \"question\".\n",
    "    # 2. Initialize your LLM using Ollama with the gemma3:1b model and a low temperature (e.g., 0.1) for reliable, grounded responses.\n",
    "    # 3. Build a ConversationalRetrievalChain by combining the LLM, the retriever, and your custom prompt and name it \"chain\".\n",
    "\n",
    "prompt = PromptTemplate(template=SYSTEM_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, combine_docs_chain_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6c7fc",
   "metadata": {},
   "source": [
    "When you ask a question, the retriever pulls the top few relevant text chunks, the model reads them through the system prompt, and then it generates an answer based on that context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1eb44d",
   "metadata": {},
   "source": [
    "### 5.3‚ÄØ-‚ÄØValidate the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee50de",
   "metadata": {},
   "source": [
    "We run a few questions to make sure everything behaves as expecte. Experiment by adding you own questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"If I'm not happy with my purchase, what is your refund policy and how do I start a return?\",\n",
    "    \"How long will delivery take for a standard order, and where can I track my package once it ships?\",\n",
    "    \"What's the quickest way to contact your support team, and what are your operating hours?\",\n",
    "]\n",
    "\n",
    "# Expected steps:\n",
    "    # 1. Initialize an empty chat_history list.\n",
    "    # 2. Loop through test_questions, pass each question and the current chat history to the chain, and append the new answer.\n",
    "    # 3. Print each question and the LLM's response to verify it's working correctly.\n",
    "\n",
    "chat_history = []\n",
    "for question in test_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    result = chain({\"question\": question, \"chat_history\": chat_history})\n",
    "    answer = result[\"answer\"]\n",
    "    print(f\"üí¨ Answer: {answer}\")\n",
    "    chat_history.append((question, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8c6b4",
   "metadata": {},
   "source": [
    "### 6‚ÄØ-‚ÄØBuild the Streamlit UI (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc640b",
   "metadata": {},
   "source": [
    "The goal here is to create a tiny demo so you can interact with your RAG system. The focus is not on UI design. We will build a very small interface only to demonstrate the end-to-end flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(page_title=\"Everstorm Support Chat\", page_icon=\"üõçÔ∏è\")\n",
    "st.title(\"üõçÔ∏è Everstorm Outfitters Support\")\n",
    "st.caption(\"Ask me anything about our policies, shipping, or returns!\")\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a **Customer Support Chatbot**. Use only the information in CONTEXT to answer.\n",
    "If the answer is not in CONTEXT, respond with \"I'm not sure from the docs.\"\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY the provided <context> to answer.\n",
    "2) If the answer is not in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "3) Be concise and accurate. Prefer quoting key phrases from the context.\n",
    "4) When possible, cite sources as [source: source] using the metadata.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Load RAG components (cached to avoid reloading)\n",
    "@st.cache_resource\n",
    "def load_rag_chain():\n",
    "    # Load embeddings\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    \n",
    "    # Load FAISS index\n",
    "    vectordb = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "    \n",
    "    # Create prompt and chain\n",
    "    prompt = PromptTemplate(template=SYSTEM_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        retriever=retriever, \n",
    "        combine_docs_chain_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "# Initialize chain\n",
    "chain = load_rag_chain()\n",
    "\n",
    "# Initialize chat history in session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask about our refund policy, shipping times, or support hours...\"):\n",
    "    # Add user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            result = chain({\"question\": prompt, \"chat_history\": st.session_state.chat_history})\n",
    "            answer = result[\"answer\"]\n",
    "            st.markdown(answer)\n",
    "    \n",
    "    # Add assistant message\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    st.session_state.chat_history.append((prompt, answer))\n",
    "\n",
    "# Sidebar with info\n",
    "with st.sidebar:\n",
    "    st.header(\"About\")\n",
    "    st.info(\"This chatbot answers questions about Everstorm Outfitters using RAG (Retrieval-Augmented Generation).\")\n",
    "    \n",
    "    if st.button(\"Clear Chat History\"):\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.chat_history = []\n",
    "        st.rerun()\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.caption(\"Powered by Gemma 3 (1B) via Ollama\")\n",
    "\n",
    "# Save this to app.py\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(\"\"\"import streamlit as st\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(page_title=\"Everstorm Support Chat\", page_icon=\"üõçÔ∏è\")\n",
    "st.title(\"üõçÔ∏è Everstorm Outfitters Support\")\n",
    "st.caption(\"Ask me anything about our policies, shipping, or returns!\")\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_TEMPLATE = \\\"\\\"\\\"\n",
    "You are a **Customer Support Chatbot**. Use only the information in CONTEXT to answer.\n",
    "If the answer is not in CONTEXT, respond with \"I'm not sure from the docs.\"\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY the provided <context> to answer.\n",
    "2) If the answer is not in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "3) Be concise and accurate. Prefer quoting key phrases from the context.\n",
    "4) When possible, cite sources as [source: source] using the metadata.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER:\n",
    "{question}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "# Load RAG components (cached to avoid reloading)\n",
    "@st.cache_resource\n",
    "def load_rag_chain():\n",
    "    # Load embeddings\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    \n",
    "    # Load FAISS index\n",
    "    vectordb = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "    \n",
    "    # Create prompt and chain\n",
    "    prompt = PromptTemplate(template=SYSTEM_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        retriever=retriever, \n",
    "        combine_docs_chain_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "# Initialize chain\n",
    "chain = load_rag_chain()\n",
    "\n",
    "# Initialize chat history in session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask about our refund policy, shipping times, or support hours...\"):\n",
    "    # Add user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            result = chain({\"question\": prompt, \"chat_history\": st.session_state.chat_history})\n",
    "            answer = result[\"answer\"]\n",
    "            st.markdown(answer)\n",
    "    \n",
    "    # Add assistant message\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    st.session_state.chat_history.append((prompt, answer))\n",
    "\n",
    "# Sidebar with info\n",
    "with st.sidebar:\n",
    "    st.header(\"About\")\n",
    "    st.info(\"This chatbot answers questions about Everstorm Outfitters using RAG (Retrieval-Augmented Generation).\")\n",
    "    \n",
    "    if st.button(\"Clear Chat History\"):\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.chat_history = []\n",
    "        st.rerun()\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.caption(\"Powered by Gemma 3 (1B) via Ollama\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ app.py created! Run: streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc8ad3",
   "metadata": {},
   "source": [
    "Run `streamlit run app.py` from your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929eabbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
